{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_trials.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1Jcz7WOe_fslQ4quJ6thXjoeq8HkC1vmn","authorship_tag":"ABX9TyNHWDuuJ6Ix/n2kGXSIZxFB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFGOa3c6D_OS","executionInfo":{"status":"ok","timestamp":1610502179935,"user_tz":420,"elapsed":6748,"user":{"displayName":"Jimmy Gammell","photoUrl":"","userId":"00039398337464321879"}},"outputId":"3f06536f-921b-4ccb-835b-adddf3281f93"},"source":["import os\r\n","print(os.getcwd())\r\n","os.chdir(os.path.join('/', 'content', 'drive', 'MyDrive', 'eqp_revised'))\r\n","print(os.getcwd())\r\n","from framework import eqp\r\n","from framework import datasets\r\n","from matplotlib import pyplot as plt\r\n","import torch\r\n","import numpy as np\r\n","import time\r\n","import pickle\r\n","filepath = os.path.join(os.getcwd(), 'results', 'mnist_verification_3layerSW')\r\n","import sys\r\n","old_stdout = sys.stdout\r\n","log_file = open(os.path.join(filepath, 'log.txt'), 'w')\r\n","sys.stdout = log_file"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/MyDrive/eqp_revised\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y97xYHYVCzHG"},"source":["topology = \\\r\n","{\r\n","    'layer sizes': [28**2, 500, 500, 500, 10],\r\n","    'network type': 'SW_intra',\r\n","    'bypass p': .0756,\r\n","    'bypass mag': .05\r\n","}\r\n","hyperparameters = \\\r\n","{\r\n","    'learning rate': .05,\r\n","    'epsilon': .5,\r\n","    'beta': 1.0,\r\n","    'free iterations': 500,\r\n","    'weakly clamped iterations': 8\r\n","}\r\n","configuration = \\\r\n","{\r\n","    'batch size': 20,\r\n","    'device': 'cuda',\r\n","    'seed': 0\r\n","}\r\n","\r\n","per_layer_rates = []\r\n","correction_matrices = []\r\n","training_errors = []\r\n","test_errors = []\r\n","\r\n","n_epochs = 250\r\n","rate_period = 1\r\n","correction_period = 10\r\n","Network = eqp.Network(topology, hyperparameters, configuration, datasets.MNIST)\r\n","\r\n","initial_W = Network.W.clone().cpu().squeeze().numpy()\r\n","initial_W_mask = Network.W.clone().cpu().squeeze().numpy()\r\n","\r\n","with open(os.path.join(filepath, 'init.pickle'), 'wb') as F:\r\n","    pickle.dump({\r\n","            'topology': topology,\r\n","            'hyperparameters': hyperparameters,\r\n","            'configuration': configuration,\r\n","            'dataset': Network.dataset.name,\r\n","            'training parameters': {'number of epochs': n_epochs, 'rate period': rate_period, 'correction_period': correction_period},\r\n","            'initial weight': initial_W,\r\n","            'initial mask': initial_W_mask}, F)\r\n","\r\n","for epoch_idx in np.arange(n_epochs):\r\n","    print('Starting epoch %d.'%(epoch_idx+1))\r\n","    t0 = time.time()\r\n","    Network.train_epoch()\r\n","    Network.calculate_test_error()\r\n","    training_errors.append(Network.training_error)\r\n","    test_errors.append(Network.test_error)\r\n","    if epoch_idx % rate_period == 0:\r\n","        per_layer_rates.append([])\r\n","        for conn in Network.interlayer_connections:\r\n","            correction = torch.norm((Network.dW*conn)/torch.sqrt(torch.norm(conn, p=1)))\r\n","            per_layer_rates[-1].append(float(correction.cpu()))\r\n","    if epoch_idx % correction_period == 0:\r\n","        correction_matrices.append(Network.dW.clone().cpu().squeeze().numpy())\r\n","    print('\\tDone.')\r\n","    print('\\tTime taken:', (time.time()-t0))\r\n","    print('\\tTraining error:', Network.training_error)\r\n","    print('\\tTest error:', Network.test_error)\r\n","    with open(os.path.join(filepath, 'e%d.pickle'%(epoch_idx)), 'wb') as F:\r\n","        pickle.dump({\r\n","                'training error': Network.training_error,\r\n","                'test error': Network.test_error,\r\n","                'per-layer rates': per_layer_rates[-1] if (epoch_idx % rate_period == 0) else None,\r\n","                'correction matrix': correction_matrices[-1] if (epoch_idx % correction_period == 0) else None,\r\n","                'weight matrix': Network.W,\r\n","                'biases': Network.B,\r\n","                'states': Network.s,\r\n","                'persistent particles': Network.persistent_particles}, F)\r\n","\r\n","final_W = Network.W.clone().cpu().squeeze().numpy()\r\n","final_W_mask = Network.W.clone().cpu().squeeze().numpy()\r\n","mean_dW = Network.mean_dW.clone().cpu().squeeze().numpy()\r\n","\r\n","with open(os.path.join(filepath, 'final.pickle'), 'wb') as F:\r\n","    pickle.dump({\r\n","            'final weight': final_W,\r\n","            'final mask': final_W_mask,\r\n","            'mean dW': mean_dW}, F)\r\n","sys.stdout = old_stdout\r\n","log_file.close()\r\n"],"execution_count":null,"outputs":[]}]}